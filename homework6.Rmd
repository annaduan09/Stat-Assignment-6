---
title: "Homework6"
author: "Trevor Kapuvari"
date: "2023-12-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r setup}
options(scipen=999)

library(wordcloud)
library(text)
library(tm)
library(SnowballC)
library(words)
library(NbClust)
library(stringr)
library(dplyr)
library(syuzhet)
library(devtools)
library(tm)
library(docxtractr)
library(stringr)
library(qdapTools)
#library(gptchatteR)
library(data.table)
library(chatgpt)
library(httr)
library(textstem)
library(tigris)
library(tidyverse)
library(sf)
library(mapview)
library(httr)
library(viridis)
library(openai)
devtools::install_github("kstagaman/autoNumCaptions")


```

```{r URL}

url <- read_sf("https://github.com/annaduan09/Stat-Assignment-6/raw/master/park_reviews.geojson")
urltext = url$yelp.json.reviews.text
#write.table(urltext, file = "yelptext.txt")

yelptext = c("https://github.com/annaduan09/Stat-Assignment-6/raw/master/yelptext.txt")

```


```{r Corpus}

myCorpus <- tm::VCorpus(VectorSource(sapply(yelptext, readLines)))

myCorpus <- tm_map(myCorpus, content_transformer(tolower))

```


```{r tinkering}

#     Defining the toSpace function
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#     Defining the remApostrophe function
remApostrophe <- content_transformer(function(x,pattern) gsub(pattern, "", x))
#     Removing special characters
myCorpus <- tm_map(myCorpus, toSpace, "@")
myCorpus <- tm_map(myCorpus, toSpace, "/")
myCorpus <- tm_map(myCorpus, toSpace, "]")
myCorpus <- tm_map(myCorpus, toSpace, "$")
myCorpus <- tm_map(myCorpus, toSpace, "—")
myCorpus <- tm_map(myCorpus, toSpace, "‐")
myCorpus <- tm_map(myCorpus, toSpace, "”")
myCorpus <- tm_map(myCorpus, toSpace, "‘")
myCorpus <- tm_map(myCorpus, toSpace, "“")
myCorpus <- tm_map(myCorpus, toSpace, "‘")
myCorpus <- tm_map(myCorpus, remApostrophe, "’")
stopwords("english")
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
myCorpus <- tm_map(myCorpus, stemDocument)
tdm <- TermDocumentMatrix(myCorpus)


```


```{r view}
myCorpus <- tm::tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removePunctuation)

tdm <- TermDocumentMatrix(myCorpus)
tm::inspect(tdm)

m<- as.matrix(tdm)
dim(m)

```

```{r view}
rownames(m) <- tdm$dimnames$Terms
#colnames(m) <- c("parks, text, frequency") this is where we run into problems because of the comparison to the rmd 


#colnames(m) <- c("War and Peace", "Crime and Punishment", "Pride and Prejudice", "Tale of Two Cities", "Emma", "Brothers Karamazov", "Jane Eyre", 
#"The Great Gatsby", "Wuthering Heights", "Moby Dick", "Call of the Wild", "Frankenstein", "Little Women", "Vanity Fair", "Anna Karenina", 
#"Count of Monte Cristo", "Great Expectations", "Les Miserables", "Dracula", "Madame Bovary", "Don Quixote", "Gullivers Travels", "Huckleberry Finn", 
#"Picture of Dorian Gray", "White Fang", "A Christmas Carol", "The Idiot", "The Time Machine", "Age of Innocence", "Of Human Bondage", "Paradise Lost", 
#"Robinson Crusoe", "Candide", "Last of the Mohicans", "Dead Souls", "Scarlet Letter", "Treasure Island", "Ulysses", "The Trial", "The Three Musketeers", "The #Metamorphosis", "Faust", "The Prince", "Leviathan", "David Copperfield", "Notre Dame de Paris", "Utopia")



head(m)

```



```{r view}

dictionary <- as.character(words::words$word)
row_names <- rownames(m)
in_dictionary <- row_names %in% dictionary
remove <- as.character(row_names[!in_dictionary])

#Since the data are so large, if we try to remove all words at once, we get an error. So we will remove them in chunks of 1000.

num_observations <- as.numeric(length(remove))  # Total number of observations
chunk_size <- 1000                              # Number of observations to display at a time

for (i in seq(1, num_observations, chunk_size)) {
  start <- i
  end <- i + chunk_size - 1
  end <- ifelse(end > num_observations, num_observations, end)
  myCorpus <- tm_map(myCorpus, removeWords, remove[start:end])  
}

```

```{r Document Term Matrix}
dtm_cleaned <- DocumentTermMatrix(myCorpus)
tm::inspect(dtm_cleaned)

```



```{r Document Term Matrix2}
m <- as.matrix(dtm_cleaned)
dim(m)
colnames(m) <- dtm_cleaned$dimnames$Terms
# compared to the RMD by Eugene, its flipped here 

# rownames(m) <- c("War and Peace", "Crime and Punishment", "Pride and Prejudice", "Tale of Two Cities", "Emma", "Brothers Karamazov", "Jane Eyre", 
#"The Great Gatsby", "Wuthering Heights", "Moby Dick", "Call of the Wild", "Frankenstein", "Little Women", "Vanity Fair", "Anna Karenina", 
#"Count of Monte Cristo", "Great Expectations", "Les Miserables", "Dracula", "Madame Bovary", "Don Quixote", "Gullivers Travels", "Huckleberry Finn", 
#"Picture of Dorian Gray", "White Fang", "A Christmas Carol", "The Idiot", "The Time Machine", "Age of Innocence", "Of Human Bondage", "Paradise Lost", 
#"Robinson Crusoe", "Candide", "Last of the Mohicans", "Dead Souls", "Scarlet Letter", "Treasure Island", "Ulysses", "The Trial", "The Three Musketeers", "The # Metamorphosis", "Faust", "The Prince", "Leviathan", "David Copperfield", "Notre Dame de Paris", "Utopia")



```






```{r DTM 3}
cs <- as.matrix(colSums(m))             #How many times each term appears across all documents (texts)
rownames(cs) <- dtm_cleaned$dimnames$Terms

hist(cs, breaks=100) 


```

```{r DTM 3}
cs <- as.matrix(colSums(m))             #How many times each term appears across all documents (texts)
rownames(cs) <- dtm_cleaned$dimnames$Terms

hist(cs, breaks=100) 


```


```{r sentiment analysis}

nrc <- syuzhet::get_sentiment_dictionary(dictionary="nrc")
afinn <- syuzhet::get_sentiment_dictionary(dictionary="afinn")
bing <- syuzhet::get_sentiment_dictionary(dictionary="bing")
syuzhet <- syuzhet::get_sentiment_dictionary(dictionary="syuzhet")
get_nrc_sentiment("flaccid")


```


```{r Dataframe Creartion}
Parks <- data.frame(Term = colnames(m), stringsAsFactors = FALSE)
Parks$Term_Frequency <- colSums(m)
#Parks <- as.data.frame(m[1,])
#Parks$Term <- as.vector(rownames(Parks))
#colnames(Parks)[1] = "Term_Frequency"
#rownames(Parks) <- 1:nrow(Parks)

nrc_sentiment <- get_nrc_sentiment(Parks$Term)

Parks_Sentiment <- cbind(Parks, nrc_sentiment)
cols_to_multiply <- names(Parks_Sentiment)[3:12]

# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Parks_Sentiment[, cols_to_multiply] <- Parks_Sentiment[, cols_to_multiply] * Parks_Sentiment$Term_Frequency


Parks_Sentiment_Total <- t(as.matrix(colSums(Parks_Sentiment[,-1:-2])))
barplot(Parks_Sentiment_Total, las=2, ylab='Count', main='Sentiment Scores')

```


```{r diff dictionary scores}
#syuzhet
Parks$Syuzhet <- as.matrix(get_sentiment(Parks$Term, method="syuzhet"))
hist(Parks$Syuzhet)

#bing
Parks$Bing <- as.matrix(get_sentiment(Parks$Term, method="bing"))
hist(Parks$Bing)

#AFINN
Parks$AFINN <- as.matrix(get_sentiment(Parks$Term, method="afinn"))
hist(Parks$AFINN)

#NRC
Parks$NRC <- as.matrix(get_sentiment(Parks$Term, method="nrc"))
hist(Parks$NRC)

```

**looks like we are seeing tendency towards positive sentiment using each of the four dictionaries, but still vast majority neutral terms

```{r sentiment summary}
sentiment_columns <- Parks[ , 3:6]
sentiment_columns <- data.frame(lapply(sentiment_columns, sign))
sentiment_columns <- data.frame(lapply(sentiment_columns, as.factor))

#Raw frequencies
sapply(sentiment_columns, function(x) if("factor" %in% class(x)) {table(x)})

#Proportions
sapply(sentiment_columns, function(x) if("factor" %in% class(x)) {prop.table(table(x))})
```

```{r word removing}
#myCorpus <- tm_map(myCorpus, removeWords,c("park"))



```


```{r word cloud, warning=FALSE}

# may provide different results each run, look for filler/uninformative words and use the word removing chunk for the list. 

tab <- as.matrix(table(cs))
wordcloud(myCorpus, min.freq=1000)


```



## ChatGPT

```{r gpt setup}
my_API <- "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"

hey_chatGPT <- function(answer_my_question) {
  chat_GPT_answer <- POST(
    url = "https://api.openai.com/v1/chat/completions",
    add_headers(Authorization = paste("Bearer", my_API)),
    content_type_json(),
    encode = "json",
    body = list(
      model = "gpt-3.5-turbo-0301",
      messages = list(
        list(
          role = "user",
          content = answer_my_question
        )
      )
    )
  )
  paste(str_trim(httr::content(chat_GPT_answer)$choices[[1]]$message$content), "TOKENS USED: ", httr::content(chat_GPT_answer)$usage$total_tokens)
}

urltext.df <- as.data.frame(urltext)

urltext.sample <- data.frame(urltext = urltext.df[sample(nrow(urltext.df), size=10), ])

urltext.sample$summary <- NULL
for (x in 1:nrow(urltext.sample)) {
  urltext.sample$summary[[x]] <- paste(hey_chatGPT(paste("Please read the following reviews of parks. For each review, what was the reviewer's main point, and can you suggest a way to improve the park based on the review?", urltext.sample$urltext[[x]])))
}

```
